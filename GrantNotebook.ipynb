{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T00:20:31.235456Z",
     "start_time": "2019-10-25T00:20:31.226453Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gjber\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gjber\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gjber\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T00:16:18.268023Z",
     "start_time": "2019-10-25T00:16:18.264073Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_path = '../Data 401 Project 2/aclImdb/train/pos/'\n",
    "negative_path = '../Data 401 Project 2/aclImdb/train/neg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T00:16:19.343779Z",
     "start_time": "2019-10-25T00:16:19.306130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts in positive 12500\n",
      "Number of texts in negative 12500\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of texts in positive\",len(os.listdir(positive_path)))\n",
    "print(\"Number of texts in negative\",len(os.listdir(negative_path)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the actual feature engineering. This can go many ways:\n",
    "\n",
    "1. bag of words\n",
    "2. word2vec\n",
    "3. tri-gram\n",
    "4. etc.\n",
    " \n",
    "However, the overarching goal of this project is to classify the sentiment of text using linear classifiers.\n",
    "As such, it appears that a bag of words approach will catch many important predictors (words like good, bad, love, hate etc.). However, this type of analysis will severely inflate the number of variables in the model. \n",
    "\n",
    "Possible work arounds could be stemming (or lemmatizing) words, removing stop words (these usually capture style and not sentiment), and only using words as predictors if they are in the top quartile of word frequencies or something of that nature.\n",
    "\n",
    "Looking out for more succinct and creative ways to capture this sentiment is also perhaps an avenue worth pursuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T00:16:26.341532Z",
     "start_time": "2019-10-25T00:16:21.847143Z"
    }
   },
   "outputs": [],
   "source": [
    "#read text files from train folder\n",
    "pos_train_txt = []\n",
    "pos_train_label = []\n",
    "\n",
    "for file_name in os.listdir(positive_path):\n",
    "#     if file_name == '.ipynb_checkpoints':\n",
    "#         continue\n",
    "    data = open(positive_path + file_name, encoding='utf-8').read()\n",
    "    pos_train_txt.append(data)\n",
    "    pos_train_label.append('pos')\n",
    "    \n",
    "neg_train_txt = []\n",
    "neg_train_label = []\n",
    "for file_name in os.listdir(negative_path):\n",
    "#     if file_name == '.ipynb_checkpoints':\n",
    "#         continue\n",
    "    data = open(negative_path + file_name, encoding='utf-8').read()\n",
    "    neg_train_txt.append(data)\n",
    "    neg_train_label.append('neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T00:27:30.599938Z",
     "start_time": "2019-10-25T00:27:30.565033Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  Bromwell High is a cartoon comedy. It ran at t...   pos\n",
       "1  Homelessness (or Houselessness as George Carli...   pos\n",
       "2  Brilliant over-acting by Lesley Ann Warren. Be...   pos\n",
       "3  This is easily the most underrated film inn th...   pos\n",
       "4  This is not the typical Mel Brooks film. It wa...   pos"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pandas dataframe from the text\n",
    "train_pos = pd.DataFrame({'text':pos_train_txt,'label':pos_train_label})\n",
    "train_neg = pd.DataFrame({'text':neg_train_txt,'label':neg_train_label})\n",
    "train = train_pos.append(train_neg)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T00:31:22.408336Z",
     "start_time": "2019-10-25T00:27:35.762887Z"
    }
   },
   "outputs": [],
   "source": [
    "# sentiment analysis\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "train['score'] = train.text.apply(lambda x: nltk.sent_tokenize(x))\n",
    "train.score = train.score.apply(lambda x: [analyzer.polarity_scores(sentence)['compound'] for sentence in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T00:31:24.542312Z",
     "start_time": "2019-10-25T00:31:24.000171Z"
    }
   },
   "outputs": [],
   "source": [
    "#taking the mean sentiment of sentence\n",
    "train.score = train.score.apply(lambda x:  np.mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T00:33:01.467770Z",
     "start_time": "2019-10-25T00:31:26.123402Z"
    }
   },
   "outputs": [],
   "source": [
    "# Word tokenize first\n",
    "train.text = train.text.apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T00:33:06.379734Z",
     "start_time": "2019-10-25T00:33:02.914868Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words())\n",
    "# Remove stop words and lower case remaining\n",
    "# Note that order of sentence is lost in this implementation\n",
    "# The set difference is much faster than alternatives for removing stop words, though\n",
    "train.text = train.text.apply(lambda x: list(set(x).difference(stop_words)))\n",
    "train.text = train.text.apply(lambda x: [word.lower() for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T00:33:07.839564Z",
     "start_time": "2019-10-25T00:33:07.827149Z"
    }
   },
   "outputs": [],
   "source": [
    "save_train = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T00:33:40.048533Z",
     "start_time": "2019-10-25T00:33:40.032062Z"
    }
   },
   "outputs": [],
   "source": [
    "# First put the txt in a format that sklearn's CountVectorizer can use it\\n\n",
    "train['joinedtxt'] = train.text.apply(lambda x: ' '.join(x))\n",
    "train['listtxt'] = train.joinedtxt.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T00:34:03.485302Z",
     "start_time": "2019-10-25T00:33:57.298265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000000000001', '00001', '00015']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(train.listtxt)\n",
    "print(vectorizer.get_feature_names()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T00:34:25.278232Z",
     "start_time": "2019-10-25T00:34:20.633867Z"
    }
   },
   "outputs": [],
   "source": [
    "bag_of_words_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T00:35:31.708766Z",
     "start_time": "2019-10-25T00:34:57.833946Z"
    }
   },
   "outputs": [],
   "source": [
    "keep_columns = bag_of_words_df.columns[bag_of_words_df.sum(axis = 0) > 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T00:35:42.819322Z",
     "start_time": "2019-10-25T00:35:42.800998Z"
    }
   },
   "outputs": [],
   "source": [
    "keep_columns = list(keep_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T00:37:33.276709Z",
     "start_time": "2019-10-25T00:35:52.622514Z"
    }
   },
   "outputs": [],
   "source": [
    "train[keep_columns] = bag_of_words_df[keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T00:37:49.401724Z",
     "start_time": "2019-10-25T00:37:49.282989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>joinedtxt</th>\n",
       "      <th>listtxt</th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>...</th>\n",
       "      <th>york</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>your</th>\n",
       "      <th>youth</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "      <td>0</td>\n",
       "      <td>teachers 'm . pathetic my student pity ... fin...</td>\n",
       "      <td>teachers 'm . pathetic my student pity ... fin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "      <td>0</td>\n",
       "      <td>young pictures fumes written already cause div...</td>\n",
       "      <td>young pictures fumes written already cause div...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "      <td>0</td>\n",
       "      <td>scenes , scene lawyer says second all-time war...</td>\n",
       "      <td>scenes , scene lawyer says second all-time war...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "      <td>0</td>\n",
       "      <td>truly traditionally . easily my someone homele...</td>\n",
       "      <td>truly traditionally . easily my someone homele...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>pos</td>\n",
       "      <td>0</td>\n",
       "      <td>without fantastic . there audience followable ...</td>\n",
       "      <td>without fantastic . there audience followable ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3770 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   text label  score                                          joinedtxt  \\\n",
       "0     0   pos      0  teachers 'm . pathetic my student pity ... fin...   \n",
       "1     0   pos      0  young pictures fumes written already cause div...   \n",
       "2     0   pos      0  scenes , scene lawyer says second all-time war...   \n",
       "3     0   pos      0  truly traditionally . easily my someone homele...   \n",
       "4     0   pos      0  without fantastic . there audience followable ...   \n",
       "\n",
       "                                             listtxt  000  10  100  11  12  \\\n",
       "0  teachers 'm . pathetic my student pity ... fin...    0   0    0   0   0   \n",
       "1  young pictures fumes written already cause div...    0   0    0   0   0   \n",
       "2  scenes , scene lawyer says second all-time war...    0   0    0   0   0   \n",
       "3  truly traditionally . easily my someone homele...    0   0    0   0   0   \n",
       "4  without fantastic . there audience followable ...    0   0    0   0   0   \n",
       "\n",
       "   ...  york  you  young  younger  your  youth  zero  zombie  zombies  zone  \n",
       "0  ...     0    0      0        0     0      0     0       0        0     0  \n",
       "1  ...     0    0      1        0     0      0     0       0        0     0  \n",
       "2  ...     0    0      0        0     0      0     0       0        0     0  \n",
       "3  ...     0    1      0        0     0      0     0       0        0     0  \n",
       "4  ...     0    0      0        0     0      0     0       0        0     0  \n",
       "\n",
       "[5 rows x 3770 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-25T00:41:32.551465Z",
     "start_time": "2019-10-25T00:38:04.210262Z"
    }
   },
   "outputs": [],
   "source": [
    "train.to_csv('initial_feature_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lemmatize the remaining words\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# train.text = train.text.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.666334,
   "position": {
    "height": "40px",
    "left": "1010px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
