{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T20:08:29.697536Z",
     "start_time": "2019-10-23T20:08:29.639808Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T20:01:52.264619Z",
     "start_time": "2019-10-23T20:01:52.256630Z"
    }
   },
   "outputs": [],
   "source": [
    "positive_path = '/data401/reviews/train/pos/'\n",
    "negative_path = '/data401/reviews/train/neg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T20:01:53.295597Z",
     "start_time": "2019-10-23T20:01:53.259775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts in positive 12501\n",
      "Number of texts in negative 12501\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of texts in positive\",len(os.listdir(positive_path)))\n",
    "print(\"Number of texts in negative\",len(os.listdir(negative_path)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the actual feature engineering. This can go many ways:\n",
    "\n",
    "1. bag of words\n",
    "2. word2vec\n",
    "3. tri-gram\n",
    "4. etc.\n",
    " \n",
    "However, the overarching goal of this project is to classify the sentiment of text using linear classifiers.\n",
    "As such, it appears that a bag of words approach will catch many important predictors (words like good, bad, love, hate etc.). However, this type of analysis will severely inflate the number of variables in the model. \n",
    "\n",
    "Possible work arounds could be stemming (or lemmatizing) words, removing stop words (these usually capture style and not sentiment), and only using words as predictors if they are in the top quartile of word frequencies or something of that nature.\n",
    "\n",
    "Looking out for more succinct and creative ways to capture this sentiment is also perhaps an avenue worth pursuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T20:06:31.815377Z",
     "start_time": "2019-10-23T20:06:28.615901Z"
    }
   },
   "outputs": [],
   "source": [
    "#read text files from train folder\n",
    "pos_train_txt = []\n",
    "pos_train_label = []\n",
    "\n",
    "for file_name in os.listdir(positive_path):\n",
    "    if file_name == '.ipynb_checkpoints':\n",
    "        continue\n",
    "    data = open(positive_path + file_name, encoding='utf-8').read()\n",
    "    pos_train_txt.append(data)\n",
    "    pos_train_label.append('pos')\n",
    "    \n",
    "neg_train_txt = []\n",
    "neg_train_label = []\n",
    "for file_name in os.listdir(negative_path):\n",
    "    if file_name == '.ipynb_checkpoints':\n",
    "        continue\n",
    "    data = open(negative_path + file_name, encoding='utf-8').read()\n",
    "    neg_train_txt.append(data)\n",
    "    neg_train_label.append('neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T20:14:38.723679Z",
     "start_time": "2019-10-23T20:14:38.685772Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The Shining is a weird example of adaptation: ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>In 1967 I visited the Lake Elsinore glider-por...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I think Hollow Point is a funny film with some...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>I notice the DVD version seems to have missing...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Men of Honor has many great aspects to it. Goo...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  The Shining is a weird example of adaptation: ...   pos\n",
       "1  In 1967 I visited the Lake Elsinore glider-por...   pos\n",
       "2  I think Hollow Point is a funny film with some...   pos\n",
       "3  I notice the DVD version seems to have missing...   pos\n",
       "4  Men of Honor has many great aspects to it. Goo...   pos"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pandas dataframe from the text\n",
    "train_pos = pd.DataFrame({'text':pos_train_txt,'label':pos_train_label})\n",
    "train_neg = pd.DataFrame({'text':neg_train_txt,'label':neg_train_label})\n",
    "train = train_pos.append(train_neg)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's begin by removing stop words and lemmatizing the rest.\n",
    "\n",
    "- lemmatizing over stemming is chosen because it should produce a smaller subset of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In 1967 I visited the Lake Elsinore glider-port and flew a yellow Pratt Read sailplane. Returning to Germany the above serious ran on TV and one segment was about the high altitude sailplane flights in California in the early 50ies. (The real life pilot was Bill Ivans, I don't know who played him in the series) It turned out that the sailplane in the film was the same (same N-number) as the one I had flown at Lake Elsinore. Ever since I saw that segment I have been searching for it and have been wondering if it is somewhere available. (other segments in that serious were about the Baker Ejection Seat; an instrument to find avalanche victims etc.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "train['score'] = train.text.apply(lambda x: nltk.sent_tokenize(x))\n",
    "train.score = train.score.apply(lambda x: [analyzer.polarity_scores(sentence)['compound'] for sentence in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking the mean sentiment of sentence\n",
    "\n",
    "train.score = train.score.apply(lambda x:  np.mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T20:15:35.968670Z",
     "start_time": "2019-10-23T20:14:40.822878Z"
    }
   },
   "outputs": [],
   "source": [
    "# Word tokenize first\n",
    "train.text = train.text.apply(lambda x: nltk.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-23T20:28:04.122739Z",
     "start_time": "2019-10-23T20:15:49.995100Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Remove stop words\n",
    "train.text = train.text.apply(lambda x: [word for word in x if word not in stopwords.words()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the remaining words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "train.text = train.text.apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.666334,
   "position": {
    "height": "40px",
    "left": "1010px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
