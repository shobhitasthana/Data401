{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes are in the github dir\n",
    "positive_path = '../data401/reviews/train/pos/'\n",
    "negative_path = '../data401/reviews/train/neg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts in positive 12500\n",
      "Number of texts in negative 12500\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of texts in positive\",len(os.listdir(positive_path)))\n",
    "print(\"Number of texts in negative\",len(os.listdir(negative_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the actual feature engineering. This can go many ways:\n",
    "\n",
    "1. bag of words\n",
    "2. word2vec\n",
    "3. tri-gram\n",
    "4. etc.\n",
    " \n",
    "However, the overarching goal of this project is to classify the sentiment of text using linear classifiers.\n",
    "As such, it appears that a bag of words approach will catch many important predictors (words like good, bad, love, hate etc.). However, this type of analysis will severely inflate the number of variables in the model. \n",
    "\n",
    "Possible work arounds could be stemming (or lemmatizing) words, removing stop words (these usually capture style and not sentiment), and only using words as predictors if they are in the top quartile of word frequencies or something of that nature.\n",
    "\n",
    "Looking out for more succinct and creative ways to capture this sentiment is also perhaps an avenue worth pursuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's begin by removing stop words and lemmatizing the rest.\n",
    "\n",
    "- lemmatizing over stemming is chosen because it should produce a smaller subset of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-34544916dba2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mnum_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_num_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mcurr_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'close'"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "ids = []\n",
    "bag_of_word = []\n",
    "avg_word_length = []\n",
    "num_word = []\n",
    "\n",
    "# Create features for positive training txts\n",
    "for file_name in os.listdir(positive_path):\n",
    "    # Open txt file\n",
    "    curr_text = open(positive_path + file_name).read()\n",
    "    ids.append(file_name)\n",
    "        \n",
    "    # Word Tokenize and remove stop words\n",
    "    curr_text = nltk.word_tokenize(curr_text)\n",
    "    curr_text = [word for word in curr_text if word not in stopwords.words()]\n",
    "    \n",
    "    # Lemmatize text\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in curr_text]\n",
    "    bag_of_word.append(lemmatized_text)\n",
    "    \n",
    "    # Get statistics about sentences\n",
    "    # This statistic captures more about style than sentiment yet still may be interesting\n",
    "    curr_avg_word_length = sum([len(word) for word in curr_text]) / len(curr_text)\n",
    "    avg_word_length.append(curr_avg_word_length)\n",
    "    curr_num_word = len(curr_text)\n",
    "    num_word.append(curr_num_word)\n",
    "        \n",
    "\n",
    "positive_feature_df = pd.DataFrame({'id':ids, 'bag_of_words':bag_of_word,'avg_word_length':avg_word_length,\n",
    "                                   'num_word':num_word})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
